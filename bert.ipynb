{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d48d14c8",
      "metadata": {
        "id": "d48d14c8"
      },
      "source": [
        "# Fine Tuning A BERT Model With HuggingFace"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f535e23",
      "metadata": {
        "id": "0f535e23"
      },
      "source": [
        "## 1. Introduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N5WtjwRliSOZ",
      "metadata": {
        "id": "N5WtjwRliSOZ"
      },
      "outputs": [],
      "source": [
        "# !pip install arxiv\n",
        "# !pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6dbfe07",
      "metadata": {
        "id": "d6dbfe07"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "pxN8Y1rv6maU",
      "metadata": {
        "id": "pxN8Y1rv6maU"
      },
      "source": [
        " Next I authenticate myself as my Google account user. This will be helpful since I will be storing the documents as json in [Google Cloud Storage](https://cloud.google.com/storage?hl=en). Authentication through [colab](https://colab.research.google.com/) means there's no extra steps or API keys for me to access the data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adSUO_bXimfj",
      "metadata": {
        "id": "adSUO_bXimfj"
      },
      "outputs": [],
      "source": [
        "import google.colab as colab\n",
        "colab.auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dfff11c",
      "metadata": {
        "id": "2dfff11c"
      },
      "source": [
        "## 2. Collecting The Data\n",
        "\n",
        "The first thing I need to do is collect data. In a [prior post](https://michael-harmon.com/blog/NLP1.html) I got documents for classification by collecting paper abstracts from [arxiv](https://arxiv.org/). I was going to reuse those same documents, but over the years I lost them. So, instead I'll use the [arixv package](https://lukasschwab.me/arxiv.py/arxiv.html) to create a new dataset for classification. Instead of 4 classes I'll use 3 and still make the datasets imbalanced. The 3 classes I'll use are the topics of the papers which I chose to be 'Artificial Intelligence', 'Information Retrieval' and 'Robotics'.\n",
        "\n",
        "First I collect 1,000 papers on 'Ariticial Intelligence', 1,000 papers on 'Information Retrieval' and 100 on 'Robotics' using a function I wrote called [get_data](utils.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d602965f",
      "metadata": {
        "id": "d602965f"
      },
      "outputs": [],
      "source": [
        "from utils import get_arxiv_data\n",
        "\n",
        "df = get_arxiv_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9oomEMA0kf7-",
      "metadata": {
        "id": "9oomEMA0kf7-"
      },
      "source": [
        "Now looking at the contents of the dataframe,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81a7184b",
      "metadata": {
        "id": "81a7184b"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f68da15",
      "metadata": {
        "id": "9f68da15"
      },
      "source": [
        "In the above results the `id` is the url of the paper, the `code` is the class label and `text` is the abstract of the paper.\n",
        "\n",
        "I want to be able to predict the category of the abstract based of the text. This means we need to convert the category into a numerical value. [Scikit-learn's LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) is the tool for the job,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac810e8b",
      "metadata": {
        "id": "ac810e8b"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "labeler  = LabelEncoder()\n",
        "df = df.assign(label=labeler.fit_transform(df[\"code\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eac685cd",
      "metadata": {
        "id": "eac685cd"
      },
      "source": [
        "Now each text has an associated numerical value in the column `label` with values based on the `code` value,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6646470",
      "metadata": {
        "id": "b6646470"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccaae73c",
      "metadata": {
        "id": "ccaae73c"
      },
      "source": [
        "The numerical value for each code is given by the order in the `classes_` attribute of the labler. This means mapping between the code and the label can be found by the following,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ec3501b",
      "metadata": {
        "id": "0ec3501b"
      },
      "outputs": [],
      "source": [
        "{v:k for k,v in enumerate(labeler.classes_)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tNSln9rTksz6",
      "metadata": {
        "id": "tNSln9rTksz6"
      },
      "source": [
        "Next I need to break the datasets into train, validation and test sets. I can do this with [Scikit-Learn's train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e582a95",
      "metadata": {
        "id": "1e582a95"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "                                            df[\"text\"],\n",
        "                                            df[\"label\"],\n",
        "                                            test_size=0.15,\n",
        "                                            random_state=42,\n",
        "                                            stratify=df[\"label\"])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train,\n",
        "                                                  y_train,\n",
        "                                                  test_size=0.20,\n",
        "                                                  random_state=42,\n",
        "                                                  stratify=y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oEGg_q3GkwQv",
      "metadata": {
        "id": "oEGg_q3GkwQv"
      },
      "source": [
        "The size of the datsets are,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df086d73",
      "metadata": {
        "id": "df086d73"
      },
      "outputs": [],
      "source": [
        "X_train.shape, X_val.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18dda8e0",
      "metadata": {
        "id": "18dda8e0"
      },
      "source": [
        "These are small datasets, but luckily using finetuning we can still build a high performance model! I know that Scikit-Learn uses stratified sampling by default, but I'll still check to make sure the distribution of class labels is consistent between the train, validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2035e2c1",
      "metadata": {
        "id": "2035e2c1"
      },
      "outputs": [],
      "source": [
        "from utils import plot_target_distribution_combined\n",
        "plot_target_distribution_combined(train_df, val_df, test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae884903",
      "metadata": {
        "id": "ae884903"
      },
      "source": [
        "We can see that it distribution of classes across each dataset is consistent. The last thing to do before modeling is combine `X` and `y` back into one dataframe and save them to [Google Cloud Storage](https://cloud.google.com/storage?hl=en). This is necessary so I can come back to this project over time and still work with the same data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fced93d",
      "metadata": {
        "id": "3fced93d"
      },
      "outputs": [],
      "source": [
        "train_df = pd.DataFrame({\"text\": X_train, \"label\": y_train})\n",
        "train_df.to_json(\"gs://harmon-arxiv/train_abstracts.json\")\n",
        "\n",
        "val_df   = pd.DataFrame({\"text\": X_val, \"label\": y_val})\n",
        "val_df.to_json(\"gs://harmon-arxiv/val_abstracts.json\")\n",
        "\n",
        "test_df =  pd.DataFrame({\"text\": X_test, \"label\": y_test})\n",
        "test_df.to_json(\"gs://harmon-arxiv/test_abstracts.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4063b997",
      "metadata": {
        "id": "4063b997"
      },
      "source": [
        "## 2. HuggingFace Datasets & Models\n",
        "\n",
        "Now that I have the data in [Google Cloud Storage](https://cloud.google.com/storage?hl=en) we begin the fine tuning of our model. Since this is a classification problem I'll use a [Encoder model](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)); specifically a Bidirectional Encoder Representations from Transformers [BERT](https://huggingface.co/docs/transformers/en/model_doc/bert) model. BERT's architecture is pictured below,\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/mdh266/FineTuning/blob/main/images/bert.png?raw=1\" alt=\"BERT\" width=\"400\" height=\"600\" class=\"center\">\n",
        "<figcaption>BERT Encoder Architecture\n",
        "</figure>\n",
        "\n",
        "I wont go over much about Encoders and Transformers as the internet has plently of good material. I found [Andrew Ng's Sequence Models](https://www.coursera.org/learn/nlp-sequence-models/paidmedia?specialization=deep-learning) course along with the [100 Page Large Language Models Book](https://www.thelmbook.com/) very helpful in understanding transformers. Instead, this post will focus on how to fine tune a BERT model for text classification using the [Hugging Face API](https://huggingface.co/). I have heard of Hugging Face for years, but never fully understood what it was. I am currently making my way through the [Hugging Face LLM Course](https://huggingface.co/learn/llm-course/chapter1/1) and figured I would solidify my learnings by writing this post. Hugging Face is an open-soure platform and api for building and sharing artificial intelligence models (as well as datasets to buildt them). It is frequently called the \"Git Hub\" of AI models. With the API you can very easily download a pre-trained model, fine tune for your problem and the push it back to their \"Model Hub\" where others in the community can use it. And I'll be doing just that in this post! The last thing I'll say about Hugging Face is that the Python library works as a high level wrapper around deep learning frameworks such as [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/) and [JAX](https://docs.jax.dev/en/latest/).\n",
        "\n",
        "The first thing I do is import Pandas (to reload the data from cloud storage) as well as the necessary [PyTorch](https://pytorch.org/) and Hugging Face modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bNCDh3hKuPY_",
      "metadata": {
        "id": "bNCDh3hKuPY_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Hugging Face imports\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
        "from datasets import Dataset, DatasetDict\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cTmcavjxuOls",
      "metadata": {
        "id": "cTmcavjxuOls"
      },
      "source": [
        "Now I can load the datasets from cloud storage as Pandas dataframes,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8271bf28",
      "metadata": {
        "id": "8271bf28"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_json(\"gs://harmon-arxiv/train_abstracts.json\")\n",
        "val_df = pd.read_json(\"gs://harmon-arxiv/val_abstracts.json\")\n",
        "test_df = pd.read_json(\"gs://harmon-arxiv/test_abstracts.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zv_uE5sG1ri9",
      "metadata": {
        "id": "Zv_uE5sG1ri9"
      },
      "source": [
        "and then convert them to [Hugging Face Datasets](https://huggingface.co/docs/datasets/en/index) so they can be used by the Hugging Face model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07628128",
      "metadata": {
        "id": "07628128"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
        "val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n",
        "test_dataset = Dataset.from_pandas(test_df, preserve_index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0Yeh1XFy1wbc",
      "metadata": {
        "id": "0Yeh1XFy1wbc"
      },
      "source": [
        "Finally I will combine them into a [DatasetDict](https://huggingface.co/docs/datasets/v4.0.0/en/package_reference/main_classes#datasets.DatasetDict) obect. This is not necessary, but it is convenient since applying a transformation to the DatasetDict applies it all the Datasets. This avoids repeating the same transformations individually across each dataset individually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e7236b2",
      "metadata": {
        "id": "2e7236b2"
      },
      "outputs": [],
      "source": [
        "dataset_dict = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"validation\": val_dataset,\n",
        "    \"test\": test_dataset\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_xLCsacu2evg",
      "metadata": {
        "id": "_xLCsacu2evg"
      },
      "source": [
        "Next I download the [BERT](https://huggingface.co/docs/transformers/en/model_doc/bert) model from [HuggingFace's Model Hub](https://huggingface.co/models) as well as its associated [Tokenizer](https://huggingface.co/docs/transformers/en/main_classes/tokenizer). To do so, I use the [AutoTokenizer and AutoModelForSequenceClassification classes](https://huggingface.co/docs/transformers/en/model_doc/auto) as they allow me to swap out models easily. Notice that the tokenizer has to match the model and we have to use the [from_pretrained class methods](https://www.geeksforgeeks.org/python/classmethod-in-python/) for each class. This ensures that the tokenizer and weights for the model are both initialized from the same point in pre-training.\n",
        "\n",
        "\n",
        "Lastly, notice move the model to the GPU and that I have to put the number of classes in AutoModelForSequenceClassification during instantiation. This adds a linear layer with softmax on top of the foundational model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VmgyHuk72XiA",
      "metadata": {
        "id": "VmgyHuk72XiA"
      },
      "outputs": [],
      "source": [
        "checkpoint = \"google-bert/bert-base-uncased\"\n",
        "device=\"cuda\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S1y2BmCAICE2",
      "metadata": {
        "id": "S1y2BmCAICE2"
      },
      "source": [
        "Note that the tokenizer here is not a word level tokenization like I have used in [prior blog posts](https://michael-harmon.com/blog/NLP1.html) that have used the [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) model. Instead it uses a [sub-word tokenization method](https://huggingface.co/learn/llm-course/chapter6/6?fw=pt). The [100 Page Large Language Models Book](https://www.thelmbook.com/) had a good explanation on this topic, albiet it focused on Byte-Pair Encoding tokenization while BERT uses a WordPiece tokenization.\n",
        "\n",
        "I can see that the model I have downloaded is a BERT MMdel by looking at its type:\n",
        "\n",
        "```\n",
        "type(model)\n",
        "```\n",
        "\n",
        "it returns,\n",
        "\n",
        "```\n",
        "transformers.models.bert.modeling_bert.BertForSequenceClassification\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IThDzD8H4kBq",
      "metadata": {
        "id": "IThDzD8H4kBq"
      },
      "source": [
        "and\n",
        "```\n",
        "print(model)\n",
        "```\n",
        "\n",
        "which will return,\n",
        "\n",
        "\n",
        "```\n",
        "BertForSequenceClassification(\n",
        "  (bert): BertModel(\n",
        "    (embeddings): BertEmbeddings(\n",
        "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
        "      (position_embeddings): Embedding(512, 768)\n",
        "      (token_type_embeddings): Embedding(2, 768)\n",
        "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "      (dropout): Dropout(p=0.1, inplace=False)\n",
        "    )\n",
        "    (encoder): BertEncoder(\n",
        "      (layer): ModuleList(\n",
        "        (0-11): 12 x BertLayer(\n",
        "          (attention): BertAttention(\n",
        "            (self): BertSdpaSelfAttention(\n",
        "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (dropout): Dropout(p=0.1, inplace=False)\n",
        "            )\n",
        "            (output): BertSelfOutput(\n",
        "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "              (dropout): Dropout(p=0.1, inplace=False)\n",
        "            )\n",
        "          )\n",
        "          (intermediate): BertIntermediate(\n",
        "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "            (intermediate_act_fn): GELUActivation()\n",
        "          )\n",
        "          (output): BertOutput(\n",
        "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "            (dropout): Dropout(p=0.1, inplace=False)\n",
        "          )\n",
        "        )\n",
        "      )\n",
        "    )\n",
        "    (pooler): BertPooler(\n",
        "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "      (activation): Tanh()\n",
        "    )\n",
        "  )\n",
        "  (dropout): Dropout(p=0.1, inplace=False)\n",
        "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
        ")\n",
        "```\n",
        "\n",
        "The \"classifier\" layer (aka the \"classification head\") is the linear that was added to the model when I downloaded it. The `out_features` parameter that shows the output has 3 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OqMVXeeF2WOQ",
      "metadata": {
        "id": "OqMVXeeF2WOQ"
      },
      "source": [
        "Now I can tokenizer the datsets by creating a `tokenize_function` and applying it to the DataDict with the `map` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EPggdDIN2Ros",
      "metadata": {
        "id": "EPggdDIN2Ros"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JklGJeCDEM0m",
      "metadata": {
        "id": "JklGJeCDEM0m"
      },
      "source": [
        "Notice that I have the parameter `batched=True`, however, we have not used any padding. I will use [Dynamic Padding](https://huggingface.co/learn/llm-course/en/chapter3/2#dynamic-padding) which will determine the maximum length of documents per batch. The maximum length of documents will determine the amount of padding to be used at a batch level. If I did not use batching with Dynamic Padding all batches would have to be read in to determine the length of the longest document to determine padding size for each document. In my case, this is not such a big deal since the dataset is already in memory, but when reading from disk it can be helpful.\n",
        "\n",
        "To use Dynamic Padding I use the [DataCollatorWithPadding](https://huggingface.co/docs/transformers/en/main_classes/data_collator) class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f5c0316",
      "metadata": {
        "id": "5f5c0316"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OnkwidEgEPXW",
      "metadata": {
        "id": "OnkwidEgEPXW"
      },
      "source": [
        "This will be used later on during training since it's just adding 0's at the beginning or end of the tokenized vector (`token_ids`) within each batch. I can see the schema of the datasets by looking at the columns:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "tokenized_datasets[\"test\"].features\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "{'text': Value('string'),\n",
        " 'label': Value('int64'),\n",
        " 'input_ids': List(Value('int32')),\n",
        " 'token_type_ids': List(Value('int8')),\n",
        " 'attention_mask': List(Value('int8'))}\n",
        " ```"
      ],
      "metadata": {
        "id": "A9zyexJoLH8L"
      },
      "id": "A9zyexJoLH8L"
    },
    {
      "cell_type": "markdown",
      "id": "f536d20f",
      "metadata": {
        "id": "f536d20f"
      },
      "source": [
        "HuggingFace requires that the datasets only have the following columns:\n",
        "\n",
        "* `labels`: The class for the text.\n",
        "\n",
        "* `input_ids`: Vector of integers for the numerical representation of tokenized.\n",
        "  \n",
        "* `attention_mask`: List of 0's or 1's for the model to infer if it should \"attend\" to this token in the attention mechanism.\n",
        "\n",
        "In order to get the dataset to meet this requirements I will drop the \"text\" column and rename the \"label\" column to \"labels\","
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e0d9e42",
      "metadata": {
        "id": "0e0d9e42"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = tokenized_datasets.remove_columns(\"text\")\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ah0uUL1EQMV",
      "metadata": {
        "id": "1ah0uUL1EQMV"
      },
      "source": [
        "Since I will be using [PyTorch](https://pytorch.org/) as a backend I have to convert the arrays in the datasets into PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29c15dfc",
      "metadata": {
        "id": "29c15dfc"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = tokenized_datasets.with_format(\"torch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "052ce088",
      "metadata": {
        "id": "052ce088"
      },
      "source": [
        "Lastly, I can confirm the schema and size of the datsets,"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "print(tokenized_datasets[\"test\"].features)\n",
        "\n",
        "{'labels': Value('int64'),\n",
        "'input_ids': List(Value('int32')),\n",
        "'token_type_ids': List(Value('int8')),\n",
        "'attention_mask': List(Value('int8'))}\n",
        "```\n",
        "and the size of the datasets\n",
        "```\n",
        "print(tokenized_datasets.num_rows)\n",
        "\n",
        "{'train': 1428, 'validation': 357, 'test': 315}\n",
        "```"
      ],
      "metadata": {
        "id": "UxEN2PKoLzPx"
      },
      "id": "UxEN2PKoLzPx"
    },
    {
      "cell_type": "markdown",
      "id": "468bf75f",
      "metadata": {
        "id": "468bf75f"
      },
      "source": [
        "## 3. Fine Tuning BERT\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "id": "z8CjIWTsJSyk"
      },
      "id": "z8CjIWTsJSyk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a multiclass [ROC-AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) metric to measure the perofrmance of the model. This is a pretty standard metric for classification problems since it in essence measures \"how well the model call separate the classes.\" Though it should be noted the ROC-AUC curve can be misleading when you have imbalanced classes as I discussed in a [prior post](https://michael-harmon.com/blog/NLP1.html).\n",
        "\n",
        "In order to use metrics to evaluate the performance of Hugging Face models users must use the [evaluate](https://huggingface.co/docs/evaluate/en/index) library from Hugging Face. I use the [one vs. rest multi-class ROC-AUC](https://huggingface.co/spaces/evaluate-metric/roc_auc). In order to pass it into the Hugging Face fine tunning library I have to define the following function:"
      ],
      "metadata": {
        "id": "PdE_4VSLNtb6"
      },
      "id": "PdE_4VSLNtb6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0f54b5a",
      "metadata": {
        "id": "e0f54b5a"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    roc_auc_score = evaluate.load(\"roc_auc\", \"multiclass\")\n",
        "    preds, labels = eval_preds\n",
        "    scores = torch.nn.functional.softmax(torch.tensor(preds), dim=-1)\n",
        "\n",
        "    return roc_auc_score.compute(prediction_scores=scores, references=labels, multi_class=\"ovr\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "L3_b0MX_b-PJ"
      },
      "id": "L3_b0MX_b-PJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "543a1b53",
      "metadata": {
        "id": "543a1b53"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    # load_best_model_at_end=True,\n",
        "    # push_to_hub=True,\n",
        "    # hub_model_id=\"mdh266/arxivist\",\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Um8LL7Xpe8EP"
      },
      "id": "Um8LL7Xpe8EP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "KsL3FgPdFL7p",
      "metadata": {
        "id": "KsL3FgPdFL7p"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch import nn\n",
        "# from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# weights = compute_class_weight(class_weight=\"balanced\", classes=train_df[\"label\"].unique(), y=train_df[\"label\"])\n",
        "\n",
        "# class WeightedTrainer(Trainer):\n",
        "#     def compute_loss(self, model, inputs, num_items_in_batch, return_outputs=False, ):\n",
        "#         labels = inputs.get(\"labels\")\n",
        "#         # forward pass\n",
        "#         outputs = model(**inputs)\n",
        "#         logits = outputs.get('logits')\n",
        "#         # compute custom loss\n",
        "#         loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(weights))\n",
        "#         loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "#         return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "xZCzecM2eRi4"
      },
      "id": "xZCzecM2eRi4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eac0ccb6",
      "metadata": {
        "id": "eac0ccb6"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = WeightedTrainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    processing_class=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GCXVJV2IcVYr"
      },
      "id": "GCXVJV2IcVYr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "K79AE2j_FNuP",
      "metadata": {
        "id": "K79AE2j_FNuP"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d0574ca",
      "metadata": {
        "id": "3d0574ca"
      },
      "outputs": [],
      "source": [
        "output = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32c887da",
      "metadata": {
        "id": "32c887da"
      },
      "outputs": [],
      "source": [
        "tokenizer = trainer.processing_class\n",
        "tokenizer.push_to_hub(\"mdh266/arxivist\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98daa73e",
      "metadata": {
        "id": "98daa73e"
      },
      "outputs": [],
      "source": [
        "model = trainer.model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.label2id = {v:k for k,v in enumerate(['Artificial Intelligence','Information Retrieval', 'Robotics'])}\n",
        "model.config.id2label = {k:v for k,v in enumerate(['Artificial Intelligence','Information Retrieval', 'Robotics'])}\n",
        "model.push_to_hub(\"mdh266/arxivist\")\n"
      ],
      "metadata": {
        "id": "g_T1ZE4COaTO"
      },
      "id": "g_T1ZE4COaTO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ZgPKkm9mIlf"
      },
      "id": "3ZgPKkm9mIlf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "G61BCBfdPDq2"
      },
      "id": "G61BCBfdPDq2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"text-classification\", model=\"mdh266/arxivist\")"
      ],
      "metadata": {
        "id": "lEgVb8nNmeII"
      },
      "id": "lEgVb8nNmeII",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://arxiv.org/abs/2508.05557\n",
        "text = \"Social media has evolved into a complex multimodal environment where text, images, and other signals interact to shape nuanced meanings, often concealing harmful intent. Identifying such intent, whether sarcasm, hate speech, or misinformation, remains challenging due to cross-modal contradictions, rapid cultural shifts, and subtle pragmatic cues. To address these challenges, we propose MV-Debate, a multi-view agent debate framework with dynamic reflection gating for unified multimodal harmful content detection. MV-Debate assembles four complementary debate agents, a surface analyst, a deep reasoner, a modality contrast, and a social contextualist, to analyze content from diverse interpretive perspectives. Through iterative debate and reflection, the agents refine responses under a reflection-gain criterion, ensuring both accuracy and efficiency. Experiments on three benchmark datasets demonstrate that MV-Debate significantly outperforms strong single-model and existing multi-agent debate baselines. This work highlights the promise of multi-agent debate in advancing reliable social intent detection in safety-critical online contexts.\""
      ],
      "metadata": {
        "id": "MYkM2D6xnGzc"
      },
      "id": "MYkM2D6xnGzc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier(text)"
      ],
      "metadata": {
        "id": "j2znvDY2oZXk"
      },
      "id": "j2znvDY2oZXk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://arxiv.org/abs/2508.05633\n",
        "text = \"Live streaming platforms have become a dominant form of online content consumption, offering dynamically evolving content, real-time interactions, and highly engaging user experiences. These unique characteristics introduce new challenges that differentiate live streaming recommendation from traditional recommendation settings and have garnered increasing attention from industry in recent years. However, research progress in academia has been hindered by the lack of publicly available datasets that accurately reflect the dynamic nature of live streaming environments. To address this gap, we introduce KuaiLive, the first real-time, interactive dataset collected from Kuaishou, a leading live streaming platform in China with over 400 million daily active users. The dataset records the interaction logs of 23,772 users and 452,621 streamers over a 21-day period. Compared to existing datasets, KuaiLive offers several advantages: it includes precise live room start and end timestamps, multiple types of real-time user interactions (click, comment, like, gift), and rich side information features for both users and streamers. These features enable more realistic simulation of dynamic candidate items and better modeling of user and streamer behaviors. We conduct a thorough analysis of KuaiLive from multiple perspectives and evaluate several representative recommendation methods on it, establishing a strong benchmark for future research. KuaiLive can support a wide range of tasks in the live streaming domain, such as top-K recommendation, click-through rate prediction, watch time prediction, and gift price prediction. Moreover, its fine-grained behavioral data also enables research on multi-behavior modeling, multi-task learning, and fairness-aware recommendation.\""
      ],
      "metadata": {
        "id": "kb3L6Q3NnGtw"
      },
      "id": "kb3L6Q3NnGtw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier(text)"
      ],
      "metadata": {
        "id": "nusZQckWoPhG"
      },
      "id": "nusZQckWoPhG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g1ymHsYCGcl2",
      "metadata": {
        "id": "g1ymHsYCGcl2"
      },
      "outputs": [],
      "source": [
        "testset_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"test\"], batch_size=8, collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "532a93dc",
      "metadata": {
        "id": "532a93dc"
      },
      "outputs": [],
      "source": [
        "def calculate_roc_auc(model, loader: DataLoader) -> Dict[str, np.float64]:\n",
        "\n",
        "  roc_auc_score = evaluate.load(\"roc_auc\", \"multiclass\")\n",
        "  model.eval()\n",
        "  for batch in loader:\n",
        "      batch = {k: v.to(device) for k, v in batch.items()}\n",
        "      with torch.no_grad():\n",
        "          outputs = model(**batch)\n",
        "          scores = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "          roc_auc_score.add_batch(references=batch[\"labels\"],\n",
        "                                prediction_scores=scores)\n",
        "\n",
        "  return roc_auc_score.compute(multi_class=\"ovr\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70353354",
      "metadata": {
        "id": "70353354"
      },
      "outputs": [],
      "source": [
        "calculate_roc_auc(model, test_dataloader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}