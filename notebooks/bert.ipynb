{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d48d14c8",
      "metadata": {
        "id": "d48d14c8"
      },
      "source": [
        "# Fine Tuning BERT With HuggingFace\n",
        "---------------------------------------------\n",
        "\n",
        "\n",
        "__[1. Introduction](#first-bullet)__\n",
        "\n",
        "__[2. Collecting Data](#second-bullet)__\n",
        "\n",
        "__[3. Hugging Face Datasets, Tokenizers & Models](#third-bullet)__\n",
        "\n",
        "__[4. Fine Tuning BERT and Hugging Face Model Hub](#fourth-bullet)__\n",
        "\n",
        "__[5. Using The Model With Hugging Face Pipelines](#fifth-bullet)__\n",
        "\n",
        "__[6. Next Steps](#sixth-bullet)__\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f535e23",
      "metadata": {
        "id": "0f535e23"
      },
      "source": [
        "## 1. Introduction <a class=\"anchor\" id=\"first-bullet\"></a>\n",
        "-----------------------------------------------------------\n",
        "\n",
        "In this notebook, I will walk through the complete process of fine-tuning a [BERT (Bidirectional Encoder Representations from Transformers)](https://en.wikipedia.org/wiki/BERT_(language_model)) model using the [HuggingFace ecosystem](https://huggingface.co/). BERT has become a cornerstone of modern NLP due to its ability to capture bidirectional context and deliver strong performance across a wide range of language understanding tasks such as classification, named entity resolution and question answering. In this post I will build of [prior posts on text classification](https://michael-harmon.com/blog/NLP4.html) by fine tuning a BERT model to classify the topic of papers in [arxiv](arxiv.org) by their abstract text. By the end of this post, I will have a working, fine-tuned BERT model ready for inference on the [Hugging Face Model Hub](https://huggingface.co/models)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "N5WtjwRliSOZ",
      "metadata": {
        "id": "N5WtjwRliSOZ"
      },
      "source": [
        "The first thing I will say is I'll be using [Google Colab](https://colab.research.google.com/) to get access to a free [CUDA](https://developer.nvidia.com/cuda-toolkit) enabled GPU. On that platform I needed install the [arxiv](https://pypi.org/project/arxiv/) and [evaluate](https://huggingface.co/docs/evaluate/en/index) libraries since they are not pre-installed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6dbfe07",
      "metadata": {
        "id": "d6dbfe07"
      },
      "outputs": [],
      "source": [
        "# !pip install arxiv\n",
        "# !pip install evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pxN8Y1rv6maU",
      "metadata": {
        "id": "pxN8Y1rv6maU"
      },
      "source": [
        " Next I authenticate myself as my Google account user. This will be helpful since I will be storing the documents as json in [Google Cloud Storage](https://cloud.google.com/storage?hl=en). Authentication through [colab](https://colab.research.google.com/) means there's no extra steps or API keys for me to access the data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adSUO_bXimfj",
      "metadata": {
        "id": "adSUO_bXimfj"
      },
      "outputs": [],
      "source": [
        "import google.colab as colab\n",
        "colab.auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dfff11c",
      "metadata": {
        "id": "2dfff11c"
      },
      "source": [
        "Now I can get started with collecting the data! Note that all the output cells have been copied to markdown cells as Colab was giving me issues with rendering the notebook on GitHub. \n",
        "\n",
        "## 2. Collecting The Data <a class=\"anchor\" id=\"second-bullet\"></a>\n",
        "--------------------------------------------------------------------\n",
        "\n",
        "The first thing I need to do is collect data. In a [prior post](https://michael-harmon.com/blog/NLP1.html) I got documents for classification by collecting paper abstracts from [arxiv](https://arxiv.org/). I was going to reuse those same documents, but over the years I lost them. So, instead I'll use the [arixv package](https://lukasschwab.me/arxiv.py/arxiv.html) to create a new dataset for classification. Instead of 4 classes I'll use 3 and still make the datasets imbalanced. The 3 classes I'll use are the topics of the papers which I chose to be 'Artificial Intelligence', 'Information Retrieval' and 'Robotics'.\n",
        "\n",
        "First I collect 1,000 papers on 'Ariticial Intelligence', 1,000 papers on 'Information Retrieval' and 100 on 'Robotics' using a function I wrote called [get_data](utils.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d602965f",
      "metadata": {
        "id": "d602965f"
      },
      "outputs": [],
      "source": [
        "from utils import get_arxiv_data\n",
        "\n",
        "df = get_arxiv_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HxtCHfx4o_us",
      "metadata": {
        "id": "HxtCHfx4o_us"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VV4hrHvUosv1",
      "metadata": {
        "id": "VV4hrHvUosv1"
      },
      "source": [
        "|    | id                                | code   | text                                                                            |\n",
        "|---:|:----------------------------------|:-------|:--------------------------------------------------------------------------------|\n",
        "|  0 | http://arxiv.org/abs/cs/9308101v1 | cs.AI  | Because of their occasional need to return to shallow points in a search ...       |\n",
        "|  1 | http://arxiv.org/abs/cs/9308102v1 | cs.AI  | Market price systems constitute a well-understood class of mechanisms that ...      |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f68da15",
      "metadata": {
        "id": "9f68da15"
      },
      "source": [
        "In the above results the `id` is the url of the paper, the `code` is the class label and `text` is the abstract of the paper.\n",
        "\n",
        "I want to be able to predict the category of the abstract based of the text. This means we need to convert the category into a numerical value. [Scikit-learn's LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) is the tool for the job,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac810e8b",
      "metadata": {
        "id": "ac810e8b"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "labeler  = LabelEncoder()\n",
        "df = df.assign(label=labeler.fit_transform(df[\"code\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eac685cd",
      "metadata": {
        "id": "eac685cd"
      },
      "source": [
        "Now each text has an associated numerical value in the column `label` with values based on the `code` value,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6646470",
      "metadata": {
        "id": "b6646470"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccaae73c",
      "metadata": {
        "id": "ccaae73c"
      },
      "source": [
        "|    | id                                | code   | text                                                                            |   label |\n",
        "|---:|:----------------------------------|:-------|:--------------------------------------------------------------------------------|--------:|\n",
        "|  0 | http://arxiv.org/abs/cs/9308101v1 | cs.AI  | Because of their occasional need to return to shallow points in a search ...        |       0 |\n",
        "|  1 | http://arxiv.org/abs/cs/9308102v1 | cs.AI  | Market price systems constitute a well-understood class of mechanisms that ...     |       0 |\n",
        "\n",
        "The numerical value for each code is given by the order in the `classes_` attribute of the labler. This means mapping between the code and the label can be found by the following,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ec3501b",
      "metadata": {
        "id": "0ec3501b"
      },
      "outputs": [],
      "source": [
        "{v:k for k,v in enumerate(labeler.classes_)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tNSln9rTksz6",
      "metadata": {
        "id": "tNSln9rTksz6"
      },
      "source": [
        "```\n",
        "{'cs.AI': 0, 'cs.IR': 1, 'cs.RO': 2}\n",
        "```\n",
        "\n",
        "Next I need to break the datasets into train, validation and test sets. I can do this with [Scikit-Learn's train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e582a95",
      "metadata": {
        "id": "1e582a95"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "                                            df[\"text\"],\n",
        "                                            df[\"label\"],\n",
        "                                            test_size=0.15,\n",
        "                                            random_state=42,\n",
        "                                            stratify=df[\"label\"])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train,\n",
        "                                                  y_train,\n",
        "                                                  test_size=0.20,\n",
        "                                                  random_state=42,\n",
        "                                                  stratify=y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oEGg_q3GkwQv",
      "metadata": {
        "id": "oEGg_q3GkwQv"
      },
      "source": [
        "The size of the datsets are,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df086d73",
      "metadata": {
        "id": "df086d73"
      },
      "outputs": [],
      "source": [
        "X_train.shape, X_val.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "crzPUgLWpqT-",
      "metadata": {
        "id": "crzPUgLWpqT-"
      },
      "source": [
        "```\n",
        "((1428,), (357,), (315,))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18dda8e0",
      "metadata": {
        "id": "18dda8e0"
      },
      "source": [
        "These are small datasets, but luckily using finetuning we can still build a high performance model! I know that Scikit-Learn uses stratified sampling by default, but I'll still check to make sure the distribution of class labels is consistent between the train, validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2035e2c1",
      "metadata": {
        "id": "2035e2c1"
      },
      "outputs": [],
      "source": [
        "from utils import plot_target_distribution_combined\n",
        "plot_target_distribution_combined(y_train, y_val, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NHVkoeNUp2y7",
      "metadata": {
        "id": "NHVkoeNUp2y7"
      },
      "source": [
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/mdh266/FineTuning/blob/main/images/distribution.png?raw=1\" alt=\"PERF\" width=\"550\" height=\"400\" class=\"center\">\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae884903",
      "metadata": {
        "id": "ae884903"
      },
      "source": [
        "We can see that it distribution of classes across each dataset is consistent. The last thing to do before modeling is combine `X` and `y` back into one dataframe and save them to [Google Cloud Storage](https://cloud.google.com/storage?hl=en). This is necessary so I can come back to this project over time and still work with the same data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fced93d",
      "metadata": {
        "id": "3fced93d"
      },
      "outputs": [],
      "source": [
        "train_df = pd.DataFrame({\"text\": X_train, \"label\": y_train})\n",
        "train_df.to_json(\"gs://harmon-arxiv/train_abstracts.json\")\n",
        "\n",
        "val_df   = pd.DataFrame({\"text\": X_val, \"label\": y_val})\n",
        "val_df.to_json(\"gs://harmon-arxiv/val_abstracts.json\")\n",
        "\n",
        "test_df =  pd.DataFrame({\"text\": X_test, \"label\": y_test})\n",
        "test_df.to_json(\"gs://harmon-arxiv/test_abstracts.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4063b997",
      "metadata": {
        "id": "4063b997"
      },
      "source": [
        "## 3. HuggingFace Datasets, Tokenizers & Models <a class=\"anchor\" id=\"third-bullet\"></a>\n",
        "-------------------------------------------------------------------------------------------\n",
        "\n",
        "Now that I have the data in [Google Cloud Storage](https://cloud.google.com/storage?hl=en) we begin the fine tuning of our model. Since this is a classification problem I'll use a [Encoder model](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)); specifically a Bidirectional Encoder Representations from Transformers [BERT](https://huggingface.co/docs/transformers/en/model_doc/bert) model. BERT's architecture is pictured below,\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/mdh266/FineTuning/blob/main/images/bert.png?raw=1\" alt=\"BERT\" width=\"300\" height=\"500\" class=\"center\">\n",
        "<figcaption>From https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11/\n",
        "</figure>\n",
        "\n",
        "I won't go over much about Encoders and Transformers as the internet has plently of good material. I found [Andrew Ng's Sequence Models](https://www.coursera.org/learn/nlp-sequence-models/paidmedia?specialization=deep-learning) course along with the [100 Page Large Language Models Book](https://www.thelmbook.com/) very helpful in understanding transformers. Instead, this post will focus on how to fine tune a BERT model for text classification using the [Hugging Face API](https://huggingface.co/). I have heard of Hugging Face for years, but never fully understood what it was. I am currently making my way through the [Hugging Face LLM Course](https://huggingface.co/learn/llm-course/chapter1/1) and figured I would solidify my learnings by writing this post. Hugging Face is an open-soure platform and api for building and sharing artificial intelligence models (as well as datasets to buildt them). It is frequently called the \"Git Hub\" of AI models. With the API you can very easily download a pre-trained model, fine tune for your problem and the push it back to their \"Model Hub\" where others in the community can use it. And I'll be doing just that in this post! The last thing I'll say about Hugging Face is that the Python library works as a high level wrapper around deep learning frameworks such as [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/) and [JAX](https://docs.jax.dev/en/latest/).\n",
        "\n",
        "The first thing I do is import Pandas (to reload the data from cloud storage) as well as the necessary [PyTorch](https://pytorch.org/) and Hugging Face modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bNCDh3hKuPY_",
      "metadata": {
        "id": "bNCDh3hKuPY_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Hugging Face imports\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
        "from datasets import Dataset, DatasetDict\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cTmcavjxuOls",
      "metadata": {
        "id": "cTmcavjxuOls"
      },
      "source": [
        "Now I can load the datasets from cloud storage as Pandas dataframes,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8271bf28",
      "metadata": {
        "id": "8271bf28"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_json(\"gs://harmon-arxiv/train_abstracts.json\")\n",
        "val_df = pd.read_json(\"gs://harmon-arxiv/val_abstracts.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zv_uE5sG1ri9",
      "metadata": {
        "id": "Zv_uE5sG1ri9"
      },
      "source": [
        "and then convert them to [Hugging Face Datasets](https://huggingface.co/docs/datasets/en/index) so they can be used by the Hugging Face model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07628128",
      "metadata": {
        "id": "07628128"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
        "val_dataset = Dataset.from_pandas(val_df, preserve_index=False)\n",
        "test_dataset = Dataset.from_pandas(test_df, preserve_index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0Yeh1XFy1wbc",
      "metadata": {
        "id": "0Yeh1XFy1wbc"
      },
      "source": [
        "Finally I will combine them into a [DatasetDict](https://huggingface.co/docs/datasets/v4.0.0/en/package_reference/main_classes#datasets.DatasetDict) obect. This is not necessary, but it is convenient since applying a transformation to the DatasetDict applies it all the Datasets. This avoids repeating the same transformations individually across each dataset individually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e7236b2",
      "metadata": {
        "id": "2e7236b2"
      },
      "outputs": [],
      "source": [
        "dataset_dict = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"validation\": val_dataset,\n",
        "    \"test\": test_dataset\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_xLCsacu2evg",
      "metadata": {
        "id": "_xLCsacu2evg"
      },
      "source": [
        "Next I download the [BERT](https://huggingface.co/docs/transformers/en/model_doc/bert) model from [HuggingFace's Model Hub](https://huggingface.co/models) as well as its associated [Tokenizer](https://huggingface.co/docs/transformers/en/main_classes/tokenizer). To do so, I use the [AutoTokenizer and AutoModelForSequenceClassification classes](https://huggingface.co/docs/transformers/en/model_doc/auto) as they allow me to swap out models easily. Notice that the tokenizer has to match the model and we have to use the [from_pretrained class methods](https://www.geeksforgeeks.org/python/classmethod-in-python/) for each class. This ensures that the tokenizer and weights for the model are both initialized from the same point in pre-training.\n",
        "\n",
        "\n",
        "Lastly, notice move the model to the GPU and that I have to put the number of classes in AutoModelForSequenceClassification during instantiation. This adds a linear layer with softmax on top of the foundational model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VmgyHuk72XiA",
      "metadata": {
        "id": "VmgyHuk72XiA"
      },
      "outputs": [],
      "source": [
        "checkpoint = \"google-bert/bert-base-uncased\"\n",
        "device=\"cuda\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S1y2BmCAICE2",
      "metadata": {
        "id": "S1y2BmCAICE2"
      },
      "source": [
        "Note that the tokenizer here is not a word level tokenization like I have used in [prior blog posts](https://michael-harmon.com/blog/NLP1.html) that have used the [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) model. Instead it uses a [sub-word tokenization method](https://huggingface.co/learn/llm-course/chapter6/6?fw=pt). The [100 Page Large Language Models Book](https://www.thelmbook.com/) had a good explanation on this topic, albiet it focused on Byte-Pair Encoding tokenization while BERT uses a WordPiece tokenization.\n",
        "\n",
        "I can see that the model I have downloaded is a BERT MMdel by looking at its type:\n",
        "\n",
        "```\n",
        "type(model)\n",
        "```\n",
        "\n",
        "it returns,\n",
        "\n",
        "```\n",
        "transformers.models.bert.modeling_bert.BertForSequenceClassification\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IThDzD8H4kBq",
      "metadata": {
        "id": "IThDzD8H4kBq"
      },
      "source": [
        "and\n",
        "```\n",
        "print(model)\n",
        "```\n",
        "\n",
        "which will return,\n",
        "\n",
        "\n",
        "```\n",
        "BertForSequenceClassification(\n",
        "  (bert): BertModel(\n",
        "    (embeddings): BertEmbeddings(\n",
        "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
        "      (position_embeddings): Embedding(512, 768)\n",
        "      (token_type_embeddings): Embedding(2, 768)\n",
        "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "      (dropout): Dropout(p=0.1, inplace=False)\n",
        "    )\n",
        "    (encoder): BertEncoder(\n",
        "      (layer): ModuleList(\n",
        "        (0-11): 12 x BertLayer(\n",
        "          (attention): BertAttention(\n",
        "            (self): BertSdpaSelfAttention(\n",
        "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (dropout): Dropout(p=0.1, inplace=False)\n",
        "            )\n",
        "            (output): BertSelfOutput(\n",
        "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "              (dropout): Dropout(p=0.1, inplace=False)\n",
        "            )\n",
        "          )\n",
        "          (intermediate): BertIntermediate(\n",
        "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "            (intermediate_act_fn): GELUActivation()\n",
        "          )\n",
        "          (output): BertOutput(\n",
        "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "            (dropout): Dropout(p=0.1, inplace=False)\n",
        "          )\n",
        "        )\n",
        "      )\n",
        "    )\n",
        "    (pooler): BertPooler(\n",
        "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "      (activation): Tanh()\n",
        "    )\n",
        "  )\n",
        "  (dropout): Dropout(p=0.1, inplace=False)\n",
        "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
        ")\n",
        "```\n",
        "\n",
        "The \"classifier\" layer (aka the \"classification head\") is the linear that was added to the model when I downloaded it. The `out_features` parameter that shows the output has 3 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OqMVXeeF2WOQ",
      "metadata": {
        "id": "OqMVXeeF2WOQ"
      },
      "source": [
        "Now I can tokenizer the datsets by creating a `tokenize_function` and applying it to the DataDict with the `map` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EPggdDIN2Ros",
      "metadata": {
        "id": "EPggdDIN2Ros"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JklGJeCDEM0m",
      "metadata": {
        "id": "JklGJeCDEM0m"
      },
      "source": [
        "Notice that I have the parameter `batched=True`, however, we have not used any padding. I will use [Dynamic Padding](https://huggingface.co/learn/llm-course/en/chapter3/2#dynamic-padding) which will determine the maximum length of documents per batch. The maximum length of documents will determine the amount of padding to be used at a batch level. If I did not use batching with Dynamic Padding all batches would have to be read in to determine the length of the longest document to determine padding size for each document. In my case, this is not such a big deal since the dataset is already in memory, but when reading from disk it can be helpful.\n",
        "\n",
        "To use Dynamic Padding I use the [DataCollatorWithPadding](https://huggingface.co/docs/transformers/en/main_classes/data_collator) class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f5c0316",
      "metadata": {
        "id": "5f5c0316"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OnkwidEgEPXW",
      "metadata": {
        "id": "OnkwidEgEPXW"
      },
      "source": [
        "This will be used later on during training since it's just adding 0's at the beginning or end of the tokenized vector (`token_ids`) within each batch. I can see the schema of the datasets by looking at the columns:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A9zyexJoLH8L",
      "metadata": {
        "id": "A9zyexJoLH8L"
      },
      "source": [
        "```\n",
        "tokenized_datasets[\"test\"].features\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "{'text': Value('string'),\n",
        " 'label': Value('int64'),\n",
        " 'input_ids': List(Value('int32')),\n",
        " 'token_type_ids': List(Value('int8')),\n",
        " 'attention_mask': List(Value('int8'))}\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f536d20f",
      "metadata": {
        "id": "f536d20f"
      },
      "source": [
        "HuggingFace requires that the datasets only have the following columns:\n",
        "\n",
        "* `labels`: The class for the text.\n",
        "\n",
        "* `input_ids`: Vector of integers for the numerical representation of tokenized.\n",
        "  \n",
        "* `attention_mask`: List of 0's or 1's for the model to infer if it should \"attend\" to this token in the attention mechanism.\n",
        "\n",
        "In order to get the dataset to meet this requirements I will drop the \"text\" column and rename the \"label\" column to \"labels\","
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e0d9e42",
      "metadata": {
        "id": "0e0d9e42"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = tokenized_datasets.remove_columns(\"text\")\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ah0uUL1EQMV",
      "metadata": {
        "id": "1ah0uUL1EQMV"
      },
      "source": [
        "Since I will be using [PyTorch](https://pytorch.org/) as a backend I have to convert the arrays in the datasets into PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29c15dfc",
      "metadata": {
        "id": "29c15dfc"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = tokenized_datasets.with_format(\"torch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "052ce088",
      "metadata": {
        "id": "052ce088"
      },
      "source": [
        "Lastly, I can confirm the schema and size of the datsets,"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xiIfKxbVfNql",
      "metadata": {
        "id": "xiIfKxbVfNql"
      },
      "source": [
        "```\n",
        "print(tokenized_datasets[\"test\"].features)\n",
        "ּּּ```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UxEN2PKoLzPx",
      "metadata": {
        "id": "UxEN2PKoLzPx"
      },
      "source": [
        "```\n",
        "{'labels': Value('int64'),\n",
        "'input_ids': List(Value('int32')),\n",
        "'token_type_ids': List(Value('int8')),\n",
        "'attention_mask': List(Value('int8'))}\n",
        "```\n",
        "and the size of the datasets\n",
        "```\n",
        "print(tokenized_datasets.num_rows)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RrKViNAkfVe0",
      "metadata": {
        "id": "RrKViNAkfVe0"
      },
      "source": [
        "```\n",
        "{'train': 1428, 'validation': 357, 'test': 315}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "468bf75f",
      "metadata": {
        "id": "468bf75f"
      },
      "source": [
        "## 4. Fine Tuning BERT and Hugging Face Model Hub <a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
        "-------------------------------------------------------------------------------------------\n",
        "\n",
        "Now I can finally turn to fine tuning the model to classify documents as \"Artificial Intelligence\", \"Informationl Retrieval\" or \"Robotics.\" Fine tuning is process of fixing the Encoder weights, but then updating the weights of the classification head. Fine tuning will make use of the patterns learned in during pre-training in the foundational model and use them to predict the topics of the documents.\n",
        "\n",
        "After fine tuning the model I'll upload it to the model hub. So the first thing I need to do is log in to Hugging Face Hub,\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z8CjIWTsJSyk",
      "metadata": {
        "id": "z8CjIWTsJSyk"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PdE_4VSLNtb6",
      "metadata": {
        "id": "PdE_4VSLNtb6"
      },
      "source": [
        "Next, I chose a multiclass [ROC-AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) metric to measure the perofrmance of the model. This is a pretty standard metric for classification problems since it in essence measures \"how well the model call separate the classes.\" Though it should be noted the ROC-AUC curve can be misleading when you have imbalanced classes as I discussed in a [prior post](https://michael-harmon.com/blog/NLP1.html).\n",
        "\n",
        "In order to use metrics to evaluate the performance of Hugging Face models users must use the [evaluate](https://huggingface.co/docs/evaluate/en/index) library from Hugging Face. I use the [one vs. rest multi-class ROC-AUC](https://huggingface.co/spaces/evaluate-metric/roc_auc). In order to pass it into the Hugging Face fine tunning library I have to define the following function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0f54b5a",
      "metadata": {
        "id": "e0f54b5a"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    roc_auc_score = evaluate.load(\"roc_auc\", \"multiclass\")\n",
        "    preds, labels = eval_preds\n",
        "    scores = torch.nn.functional.softmax(torch.tensor(preds), dim=-1)\n",
        "\n",
        "    return roc_auc_score.compute(prediction_scores=scores, references=labels, multi_class=\"ovr\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gXq1Uw9enRqY",
      "metadata": {
        "id": "gXq1Uw9enRqY"
      },
      "source": [
        "Since I'll be pushing the model to the [Hugging Face Model Hub](https://huggingface.co/models) I'll need to create a repo and I can do it by going to my profile and clicking on the `+ New Model` tab. I'll see the new model repo form shown below:\n",
        "\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/mdh266/FineTuning/blob/main/images/repo.png?raw=1\" alt=\"REPO\" width=\"600\" height=\"400\" class=\"center\">\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L3_b0MX_b-PJ",
      "metadata": {
        "id": "L3_b0MX_b-PJ"
      },
      "source": [
        "Now that I have created the repo, I'll need to create the model. During the fine tuning process I'll update versions of the model to the Model Hub and need to specity where to push the results. I also need to define the training parameters of fine tuning. I'll do all this in the `TrainingAgruments` object below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "543a1b53",
      "metadata": {
        "id": "543a1b53"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"mdh266/arxivist\",\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZZKQqC8rm5yW",
      "metadata": {
        "id": "ZZKQqC8rm5yW"
      },
      "source": [
        "These parameters train the model with 16 examples per batch from the training dataset and evaluate it with 8 examples per batch from the validation dataset. It checkpoints models both to the the [Hugging Face Model Hub](https://huggingface.co/models) (`push_to_hub=True`, specifically to the repo `hub_model_id=\"mdh266/arxivist\"`) as well as to the local dicetory `output_dir=./results`. The checkpointing occur at the end of each epoch (`save_strategy=\"epoch\"`) when the model is evaluated (`eval_strategy=\"epoch\"`). I'll point out the last parameter `report_to=\"none\"` turned off the auto logging to [Weights and Biases](https://wandb.ai/site/), for some reason this occured on Colab, but not on my laptop.\n",
        "\n",
        "Next the trainer needs to be defined which includes the model, tokenizer, training agruments object datasets, metrics to compute and the data collator for dynamic padding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Um8LL7Xpe8EP",
      "metadata": {
        "id": "Um8LL7Xpe8EP"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    processing_class=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KsL3FgPdFL7p",
      "metadata": {
        "id": "KsL3FgPdFL7p"
      },
      "source": [
        "Then we can begin the fine tuning process with the command below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d0574ca",
      "metadata": {
        "id": "3d0574ca"
      },
      "outputs": [],
      "source": [
        "output = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hnIwo4turDLU",
      "metadata": {
        "id": "hnIwo4turDLU"
      },
      "source": [
        "The results are below,\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/mdh266/FineTuning/blob/main/images/performances.png?raw=1\" alt=\"PERF\" width=\"500\" height=\"300\" class=\"center\">\n",
        "</figure>\n",
        "\n",
        "Finally there is one last push to the Model Hub I need to do which will upload all the metadata associated with fine tuning and create a basic [Hugging Face model card](https://huggingface.co/docs/hub/en/model-cards)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8lMvkqxgqTPX",
      "metadata": {
        "id": "8lMvkqxgqTPX"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub(\"mdh266/arxivist\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DICEOIBLsyHN",
      "metadata": {
        "id": "DICEOIBLsyHN"
      },
      "source": [
        "Now the model will predict text classes 0,1,2, however, in order to get the model to predict the class names \"Artificial Intelligence\", \"Information Retrieval\" and \"Robotics\" the model object needs to be modified and uploaded individually. So I will grab the model,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98daa73e",
      "metadata": {
        "id": "98daa73e"
      },
      "outputs": [],
      "source": [
        "model = trainer.model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7WsfuogutWTn",
      "metadata": {
        "id": "7WsfuogutWTn"
      },
      "source": [
        "In order to get the class labels I need to add the mappings between the labels and the class numbers to the model configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g_T1ZE4COaTO",
      "metadata": {
        "id": "g_T1ZE4COaTO"
      },
      "outputs": [],
      "source": [
        "model.config.label2id = {v:k for k,v in enumerate(['Artificial Intelligence','Information Retrieval', 'Robotics'])}\n",
        "model.config.id2label = {k:v for k,v in enumerate(['Artificial Intelligence','Information Retrieval', 'Robotics'])}\n",
        "# push to model hub\n",
        "model.push_to_hub(\"mdh266/arxivist\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_On1okAgtN79",
      "metadata": {
        "id": "_On1okAgtN79"
      },
      "source": [
        "I'll also upload the tokenizer as well:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G61BCBfdPDq2",
      "metadata": {
        "id": "G61BCBfdPDq2"
      },
      "outputs": [],
      "source": [
        "tokenizer = trainer.processing_class\n",
        "tokenizer.push_to_hub(\"mdh266/arxivist\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddYIhh_ltjBL",
      "metadata": {
        "id": "ddYIhh_ltjBL"
      },
      "source": [
        "## 5. Using the model With Hugging Face Pipelines <a class=\"anchor\" id=\"fifth-bullet\"></a>\n",
        "-------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "Now I can test the model out by downloading from Model Hub using the [Hugging Face Pipeline](https://huggingface.co/docs/transformers/en/main_classes/pipelines) class that bundles the tokenizer, model and post processing (to map the class numbers to class labels). This will allow end users to go from simple text to model class label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lEgVb8nNmeII",
      "metadata": {
        "id": "lEgVb8nNmeII"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"text-classification\", model=\"mdh266/arxivist\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V9R3Ro9Ztknh",
      "metadata": {
        "id": "V9R3Ro9Ztknh"
      },
      "source": [
        "Now I'll grab abstracts from the [arxiv.org](https://arxiv.org/) to test with the model I created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MYkM2D6xnGzc",
      "metadata": {
        "id": "MYkM2D6xnGzc"
      },
      "outputs": [],
      "source": [
        "# https://arxiv.org/abs/2508.06296\n",
        "# artificial intelligence\n",
        "with open(\"texts/ai.txt\", \"r\") as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j2znvDY2oZXk",
      "metadata": {
        "id": "j2znvDY2oZXk"
      },
      "outputs": [],
      "source": [
        "classifier(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v23kZNK3q1HQ",
      "metadata": {
        "id": "v23kZNK3q1HQ"
      },
      "source": [
        "```[{'label': 'Artificial Intelligence', 'score': 0.979738712310791}]```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kb3L6Q3NnGtw",
      "metadata": {
        "id": "kb3L6Q3NnGtw"
      },
      "outputs": [],
      "source": [
        "# https://arxiv.org/abs/2508.05633\n",
        "# information retrieval\n",
        "with open(\"texts/ir.txt\", \"r\") as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nusZQckWoPhG",
      "metadata": {
        "id": "nusZQckWoPhG"
      },
      "outputs": [],
      "source": [
        "classifier(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UTZfSLqEt0oA",
      "metadata": {
        "id": "UTZfSLqEt0oA"
      },
      "source": [
        "```[{'label': 'Information Retrieval', 'score': 0.9323310852050781}]```\n",
        "\n",
        "The pipeline class gives the class label (`label`) as well the probability the model gave that prediction (`score`).\n",
        "\n",
        "I can even do predictions on the test set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EE0d2y-52MFP",
      "metadata": {
        "id": "EE0d2y-52MFP"
      },
      "outputs": [],
      "source": [
        "classifier(test_df[\"text\"].sample(2).to_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4k8uuiwS2VqA",
      "metadata": {
        "id": "4k8uuiwS2VqA"
      },
      "source": [
        "```\n",
        "[{'label': 'Robotics', 'score': 0.9148617386817932},\n",
        " {'label': 'Information Retrieval', 'score': 0.9640209674835205}]\n",
        " ```\n",
        "\n",
        "Toet the ROC-AUC score on the test set, but first I need to write a function that will calculate it using the [DataLoader](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html) from PyTorch to enable dynamic padding. The function is below,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3jkaxMFo1YCi",
      "metadata": {
        "id": "3jkaxMFo1YCi"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def calculate_roc_auc(model, loader: DataLoader) -> Dict[str, np.float64]:\n",
        "\n",
        "  roc_auc_score = evaluate.load(\"roc_auc\", \"multiclass\")\n",
        "  model.eval()\n",
        "  for batch in loader:\n",
        "      batch = {k: v.to(device) for k, v in batch.items()}\n",
        "      with torch.no_grad():\n",
        "          outputs = model(**batch)\n",
        "          scores = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "          roc_auc_score.add_batch(references=batch[\"labels\"],\n",
        "                                prediction_scores=scores)\n",
        "\n",
        "  return roc_auc_score.compute(multi_class=\"ovr\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jJLfmsOt2leR",
      "metadata": {
        "id": "jJLfmsOt2leR"
      },
      "source": [
        "Since I need the actual class probabilities for all classes for ROC-AUC then I cannot use the pipeline classifier directly, but I must get the model instead,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_UldG1Vfs_kP",
      "metadata": {
        "id": "_UldG1Vfs_kP"
      },
      "outputs": [],
      "source": [
        "model = classifier.model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vkemP-hb2wkJ",
      "metadata": {
        "id": "vkemP-hb2wkJ"
      },
      "source": [
        "Then I can just use the data loader class as before and pass it to the above function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OdgrcaAB2sTG",
      "metadata": {
        "id": "OdgrcaAB2sTG"
      },
      "outputs": [],
      "source": [
        "testset_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"test\"], batch_size=8, collate_fn=data_collator\n",
        ")\n",
        "\n",
        "calculate_roc_auc(model, testset_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oG9fgWPC992U",
      "metadata": {
        "id": "oG9fgWPC992U"
      },
      "source": [
        "```\n",
        "{'roc_auc': np.float64(0.9821414141414141)}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b8e9a91",
      "metadata": {},
      "source": [
        "Pretty good ROC-AUC!!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JKO3dIIs_Fxa",
      "metadata": {
        "id": "JKO3dIIs_Fxa"
      },
      "source": [
        "## 6. Next Steps <a class=\"anchor\" id=\"sixth-bullet\"></a>\n",
        "----------------------------------------------------------\n",
        "In this notebook, I successfully fine-tuned a BERT model using the HuggingFace transformers library and achieved strong performance, with a ROC-AUC score of approximately 0.98 on the test set. This demonstrates BERT’s ability to generalize well when trained with high-quality data and an appropriate fine-tuning strategy.\n",
        "\n",
        "We covered the full pipeline from dataset preparation to model evaluation and showcased how to use the model for inference. This approach can be easily adapted to a variety of other NLP tasks such as sentiment analysis. One thing I will explore in the future is adding weights to a custom loss function through PyTorch to help deal with the imbalance of classes in the dataset.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
